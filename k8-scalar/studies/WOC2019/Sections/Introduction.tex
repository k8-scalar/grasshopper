SaaS providers aim to provide their services as cost-efficiently as possible. In this regard, they face the continuous challenge of utilizing only a minimal amount of resources while still meeting their customers' requirements. One way to reduce the amount of resources needed is by locating multiple applications on the same node. This entails new challenges, e.g., determining how to divide over-provisioned resources among the deployed applications, and how to handle resource contention in general. 

Kubernetes is a popular open source framework for managing containerized applications in a distributed environment, providing basic mechanisms for deployment, maintenance and scaling of applications~\citep{kubernetes_github}. 
A pod in Kubernetes is the smallest deployable unit of computing which can be created and managed~\citep{pod}. Containers belonging to the same application are grouped together in pods. 
The resources used by a pod can be limited, e.g., by setting its so-called requests and limits. The request is the amount of resources which it is guaranteed to get; the limit is the maximal amount of resources it can obtain~\citep{requestlimit}. When there is resource contention in a node, the resources are divided among the pods according to the relative weight of their requests. This mechanism is referred to as \textit{cpu-shares}. Another way of adjusting the resources available to an application is to scale it. Kubernetes offers default horizontal and vertical autoscalers, called the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) respectively. However, the VPA has several limitations which limits its usefulness in practice~\citep{vpa-limitations}.
%a well-known disadvantage of the VPA is that it currently requires to reschedule pods when dynamically adjusting requests or limits ~\citep{in-place-pod-resources-updates}.%\footnote{although, a design proposal to resolve this issue is being discussed~\citep{in-place-pod-resources-updates}}.

%So far, research on resource management in container-based environments has been rather scarce, as it is still relatively new compared to VM-based environments~\citep{Al-DhuraibiYahya2018EiCC}~\citep{CoutinhoEmanuel2015Eicc}. 
Kubernetes is evolving rapidly. Meanwhile, research is struggling to keep up with its development, especially since it is mostly aimed towards custom scaling techniques (e.g.,~\citep{hyscale}~\citep{caravel}). Furthermore, the Kubernetes mechanisms are poorly documented, which hampers effective use. Multiple questions may arise when configuring a Kubernetes cluster, e.g., how to choose suitable requests and limits, on which nodes to locate certain pods and how to configure an autoscaler. 
The goal of this paper is therefore to research how different Kubernetes features can be leveraged to cost-effectively manage resources in the framework while also meeting SLOs in the presence of dynamically evolving workloads. More concretely, we aim to provide answers to the following questions:
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \itemsep0em 
    \item What is the impact of different request and limit configurations?
    \item Does co-locating a high priority pod with a low priority pod affect the performance of the high priority pod?
    \item What is the performance impact of scaling an application using the HPA?
\end{itemize}

Different configurations may result in different performance gains (or losses) depending on the environment. For example, the user load may be very bursty, in which case using the oversubscription concepts or the HPA may not always lead to the expected results. Hence, the aforementioned impacts of the different mechanisms will be tested for both linearly increasing and bursty workloads. In this paper, however, only CPU intensive workloads will be examined. 
%Scaling memory bound systems is less common as performing a static optimization of needed memory often suffices. Furthermore, Kubernetes does not offer support for configuring network and disk resources. % candidate

% This paper evaluates the effects of simulating vertical scaling through the use of requests and limits and co-locating high-priority and low-priority pods, horizontal scaling using the default Kubernetes HPA, and a combination of both, in different environments. A test application deployed on a Kubernetes cluster is subjected to multiple experiments to illustrate the impact of different configurations.

% \begin{itemize}
%     \item \textbf{Requests and limits}
%     The impact of different request and limit configurations is examined by deploying another, low priority pod on the same node as the main application's pod. This impact is described by providing answers to two questions: first, how are resources divided among these pods; second, what is the impact of the co-location on the performance of the higher priority pod?     
% 		\item \textbf{The Kubernetes HPA}
%     If the user load rises to a point where SLAs get violated even if lower priority pods get throttled or evicted, applications may need to be scaled to provide the desired services. The performance impact of adding the default Kubernetes horizontal autoscaler to a cluster is illustrated. Furthermore, the effects of combining the autoscaler with the request and limit mechanisms, are described.
% \end{itemize}

% Different configurations may result in different performance gains (or losses) depending on the environment. For example, the user load may be very bursty, in which case using the oversubscription concepts or the HPA may not always lead to the expected results. Hence, the aforementioned impacts of the different mechanisms will be tested for both linearly increasing and bursty workloads. In this paper, however, only CPU intensive workloads will be examined. Scaling memory bound systems is less common as performing a static optimization of needed memory often suffices. Furthermore, Kubernetes does not offer support for configuring network and disk resources.

%\subsection{Research questions}
%This paper aims to provide insight into how different Kubernetes features can be employed to meet SLA requirements cost-efficiently. The Kubernetes features show potential to be effective tools for adaptive resource management and server consolidation purposes. Specifically, this paper intends to provide clear answers to the following questions:
%
%\begin{itemize}
%    \item What is the impact of different request and limit configurations?
%    \item Does co-locating a high priority pod with a low priority pod affect the performance of the high priority pod?
%    \item What is the performance impact of scaling an application using the HPA?
%    \item Does the type of user load change the answer to any of the previous questions?
%\end{itemize}
The remainder of the paper is structured as follows. %Section \ref{chap:background} contains a brief overview of all the used concepts, the most important ones being Kubernetes and its tools for resource management. 
\S\ref{chap:existing_research} discusses related work on resource management in cloud environments. \S\ref{chap:test_env} provides an overview of the test environment. In \S\ref{chap:results}, the experiments and their results are presented. \S\ref{chap:conclusion} concludes this paper.
