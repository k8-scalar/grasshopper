%\subsection{Summary of findings}
This paper described the effects of different resource management mechanisms offered by Kubernetes, namely resource allocation via request and limit configurations and the Horizontal Pod Autoscaler. In environments with a small amount of applications, experiments show that choosing proper requests increases cost-efficiency without major drawbacks. This was verified for a Cassandra based application and for an artificial SaaS application, as well as for both seasonal and bursty workloads. Due to an overhead introduced by running Cassandra on Kubernetes, scaling Cassandra in Kubernetes decreases performance instead of increasing it, regardless of the scaling algorithm used. The HPA performs well for an artificial SaaS application if the workload is seasonal, even if pods are co-located. For bursty workloads, other approaches may be preferred. In conclusion, despite some limitations, the scaling capabilities of Kubernetes show great potential to prevent SLA violations and increase resource cost-efficiency in container-centric environments.


% \paragraph{Request and limit configurations} 
% Experiments demonstrated that request and limit configurations greatly impact applications' performances on nodes with multiple pods. Allowing pods to use overprovisioned resources when needed effectively enables them to scale vertically without having to be rescheduled. A relatively high request gives pods a better claim to overprovisioned resources, and vice versa. Applications with a low request are allowed to use overprovisioned resources only if the other applications' workload is low. Co-locating low and high priority pods therefore increases cost-efficiency while only slightly impacting the performance of high priority applications, making the mechanism a useful tool for server consolidation purposes.

% These findings were confirmed for two applications: one Cassandra based log management service and one artificial SaaS-application. Moreover, the findings are valid for both seasonal and bursty workloads. The available resources are redivided almost instantly when there is a burst of workload. This makes the mechanism better suited to deal with bursty workloads than most horizontal scaling techniques, as starting new replicas is usually slow. 

% \paragraph{The Kubernetes HPA}
% Kubernetes introduces an overhead when running Cassandra. Experiments revealed that this overhead increases significantly as more replicas are added. This causes the performance of Cassandra to decrease rather than increase when scaling it in Kubernetes. The overhead is shown not to be caused by the HPA, but by Kubernetes itself. 

% For a seasonal workload, when scaling the artificial SaaS application using the HPA, only a minor overhead is observed. Scaling from one to two replicas causes the application's capacity to almost double. Furthermore, adding low priority pods to the node does not affect the operation of the HPA. The cost-efficiency benefit gained from setting adequate request configurations can thus also be leveraged when scaling applications using the HPA.

% With bursty workloads, experiments illustrated that using the default HPA leads to unexpected results. The HPA's scaling decisions are inconsistent: sometimes it decides to scale during a burst and sometimes it does not. At the time of writing, Kubernetes' official documentation concerning the HPA does not provide sufficient explanations for this behavior. 

% \subsection{Lessons Learned}
% Experiments illustrated that, in environments with a small amount of applications and where low priority applications do not need any guarantees, choosing proper request configurations increases cost-efficiency without major drawbacks. Due to an overhead introduced by running Cassandra on Kubernetes, scaling Cassandra in Kubernetes decreases performance instead of increasing it, regardless of the scaling algorithm used. The HPA performs well for an artificial SaaS application if the workload is seasonal. For bursty workloads or more complex environments, sophisticated approaches may be preferred. In conclusion, despite some limitations, the scaling capabilities of Kubernetes show great potential to prevent SLA violations and increase resource cost-efficiency in container-centric environments.
%
%\subsection{Future Work}
%A first possible extension to this paper is to further evaluate the HPA for bursty workloads. The cause of the HPA performing inconsistently could possibly be discovered by examining the exact scaling algorithm rather than the documentation. 
%
%A second extension could investigate the performance impact of scaling applications with a longer start-up time. In this case, the HPA may need to be configured differently to prevent SLA violations. Utilizing merely the HPA may not be sufficient to prevent SLA violations, however. Allowing high priority applications to use overprovisioned resources during scaling may prevent SLA violations when starting up a new replica is slow.
%
%A third possible extension could describe the effects of using the Kubernetes mechanisms when the workload is memory intensive. In this case, low priority pods get evicted instead of throttled. The performance impacts of request and limit configurations as well as scaling could be different. 

