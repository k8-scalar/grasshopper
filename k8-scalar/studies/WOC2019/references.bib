@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}

@misc{Statista.com,
  title = {{Size of the public cloud computing services market from 2009 to 2021}},
  howpublished = {\url{https://www.statista.com/statistics/273818/global-revenue-generated-with-cloud-computing-since-2009/}},
  note = {Accessed: 2018-11-19},
  author = {Statista.com}
}

@misc{in-place-pod-resources-updates,
  title = {{In-place Update of Pod Resources}},
  howpublished = {\url{https://github.com/kubernetes/enhancements/blob/29a22b61241b35bb280de83edc0aee40d1bd87bf/keps/sig-autoscaling/20181106-in-place-update-of-pod-resources.md}},
  note = {Accessed: 2019-09-09},
  author = {{Cloud Native Computing Foundation}}
}

@misc{SLO,
  title = {{service-level objective}},
  howpublished = {\url{https://searchitchannel.techtarget.com/definition/service-level-objective}},
  note = {Accessed: 2019-06-24},
  author = {Margaret Rouse}
}

@misc{autoscaling-design-proposals,
	title = {Kubernetes Design Documents and Proposals},
	howpublished = {\url{https://github.com/kubernetes/community/tree/master/contributors/design-proposals/}},
	language = {en},
	note = {Accessed: 2019-10-09},
	author = {{Cloud Native Computing Foundation}},
}

@article{SLO2,
author={Rastegari,Yousef and Shams,Fereidoon},
year={2015},
title={Optimal Decomposition of Service Level Objectives into Policy Assertions},
journal={TheScientificWorldJournal},
volume={2015},
pages={465074},
note={Date completed - 2016-08-09; Date created - 2016-03-11; Date revised - 2019-05-09; Last updated - 2019-05-22},
abstract={WS-agreement specifies quality objectives that each partner is obligated to provide. To meet quality objectives, the corresponding partner should apply appropriate policy assertions to its web services and adjust their parameters accordingly. Transformation of WS-CDL to WSBPEL is addressed in some related works, but neither of them considers quality aspects of transformation nor run-time adaptation. Here, in conformance with web services standards, we propose an optimal decomposition method to make a set of WS-policy assertions. Assertions can be applied to WSBPEL elements and affect their run-time behaviors. The decomposition method achieves the best outcome for a performance indicator. It also guarantees the lowest adaptation overhead by reducing the number of service reselections. We considered securities settlement case study to prototype and evaluate the decomposition method. The results show an acceptable threshold between customer satisfaction-the targeted performance indicator in our case study-and adaptation overhead.},
language={English},
url={https://search-proquest-com.kuleuven.ezproxy.kuleuven.be/docview/1772829285?accountid=17215},
}

@misc{SLA2,
  title = {{Definition - What is a Service Level Agreement or SLA?}},
  howpublished = {\url{https://tallyfy.com/service-level-agreement-sla/}},
  note = {Accessed: 2019-06-24},
  author = {Sonia Pearson}
}


@misc{SLA,
  title = {{What is a service level agreement (SLA)?}},
  howpublished = {\url{https://www.paloaltonetworks.com/cyberpedia/what-is-a-service-level-agreement-sla}},
  author = {paloaltonetworks.com},
  note = {Accessed: 2018-11-19}
}

@misc{pod,
  title = {{What is a Pod?}},
  howpublished = {\url{https://kubernetes.io/docs/concepts/workloads/pods/pod/}},
  author = {{Cloud Native Computing Foundation}},
  note = {Accessed: 2019-03-08}
}

@misc{requestlimit,
  title = {{Kubernetes best practices: Resource requests and limits}},
  howpublished = {\url{https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-resource-requests-and-limits}},
  author = {Google.com},
  note = {Accessed: 2019-03-08}
}

@misc{QoS,
  title = {{Resource Quality of Service in Kubernetes}},
  howpublished = {\url{https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md}},
  author = {Vishnu Kannan, Ananya Kumar},
  note = {Accessed: 2019-06-11}
}

@misc{Kubernetes-cpu-shares,
  title = {{How Pods with resource requests are scheduled}},
  howpublished = {\url{https://github.com/kubernetes/website/blob/release-1.14/content/en/docs/concepts/configuration/manage-compute-resources-container.md#how-pods-with-resource-limits-are-run}},
  author = {Vishnu Kannan, Ananya Kumar},
  note = {Accessed: 2019-06-13}
}

@misc{Docker-cpu-shares,
  title = {{Docker run reference}},
  howpublished = {\url{https://docs.docker.com/engine/reference/run//}},
  author = {Docker Inc.},
  note = {Accessed: 2019-06-13}
}

@misc{kubeadm,
  title = {{Creating a single master cluster with kubeadm}},
  howpublished = {\url{https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/}},
  author = {{Cloud Native Computing Foundation}},
  note = {Accessed: 2019-06-13}
}


@article{AhmadiM.2018Caao,
issn = {03538109},
journal = {Acta Informatica Medica},
pages = {24--28},
volume = {26},
publisher = {Avicena Publishing},
number = {1},
year = {2018},
title = {Capabilities and advantages of cloud computing in the implementation of Electronic Health Record},
copyright = {Copyright 2018 Elsevier B.V., All rights reserved.},
author = {Ahmadi, M. and Aslani, N.},
keywords = {Advantages ; Capabilities ; Cloud Computing ; Electronic Health Record (Ehr)},
note = {Accessed: 2018-11-19},
}

@misc{kubernetes_github,
  author = {{Cloud Native Computing Foundation}},
  title = {{Kubernetes Github} page},
  howpublished = "\url{https://github.com/kubernetes/kubernetes/}",
  note = {Accessed: 2018-11-19}
}

@misc{what_is_kubernetes,
  author = {Cloud Native Computing Foundation},
  title = {{What is Kubernetes}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/}",
  note = {Accessed: 2018-11-19}
}

@misc{openstack,
  author = {{OpenStack Foundation}},
  howpublished = "\url{https://www.openstack.org/}",
  note = {Accessed: 2019-08-13}
}


@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}

@article{NIST-Could-Def,
issn = {00010782},
abstract = {Cloud computing is still an evolving paradigm. Its definitions, use cases, underlying technologies, issues, risks, and benefits will be refined in a spirited debate by the public and private sectors. Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service-provider interaction. The capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The cloud infrastructure is operated solely for an organization.},
journal = {Association for Computing Machinery. Communications of the ACM},
volume = {53},
publisher = {Association for Computing Machinery},
number = {6},
year = {2010},
title = {The NIST Definition of Cloud Computing},
language = {eng},
address = {New York},
author = {Mell, Peter and Grance, Tim},
keywords = {United States–Us ; Cloud Computing ; Software Services ; Advantages ; United States ; Information Technology Management},
url = {http://search.proquest.com/docview/577585592/},
}

@inproceedings{hyscale,
title={HyScale: Hybrid Scaling of Dockerized Microservices Architectures},
author={Wong, Jonathon Paul and Kwan, Anthony and Jacobsen, Hans-Arno},
booktitle={Proceedings of ICDCS 2019},
year={2019},
publisher = {IEEE},

}}

@misc{virtualization,
  author = {Preethi Kasireddy},
  title = {{A Beginner-Friendly Introduction to Containers, VMs and Docker}},
  howpublished = "\url{https://medium.freecodecamp.org/a-beginner-friendly-introduction-to-containers-vms-and-docker-79a9e3e119b}",
  note = {Accessed: 2019-04-12}
}

@misc{Hypervisor,
  author = {VMWare},
  title = {{What is a Hypervisor?}},
  howpublished = "\url{https://www.vmware.com/topics/glossary/content/hypervisor}",
  note = {Accessed: 2019-04-12}
}

@misc{Container-vs-vm,
  author = {Doug Chamberlain},
  title = {{Containers vs. Virtual Machines (VMs): What's the Difference?}},
  howpublished = "\url{https://blog.netapp.com/blogs/containers-vs-vms/}",
  note = {Accessed: 2019-04-12}
}

@misc{Cgroup,
  author = {Petros Koutoupis},
  title = {{Everything You Need to Know about Linux Containers, Part I: Linux Control Groups and Process Isolation}},
  howpublished = "\url{https://www.linuxjournal.com/content/everything-you-need-know-about-linux-containers-part-i-linux-control-groups-and-process}",
  note = {Accessed: 2019-04-12}
}

@misc{Namespace,
  author = {Michael Kerrisk},
  title = {{Linux programmer's manual - namespaces}},
  howpublished = "\url{http://man7.org/linux/man-pages/man7/namespaces.7.html}",
  note = {Accessed: 2019-04-12}
}

@misc{Docker-image,
  title = {{Docker Overview}},
  author = {Docker Inc.},
  howpublished = "\url{https://docs.docker.com/engine/docker-overview/}",
  note = {Accessed: 2019-04-15}
}

@misc{Dockerfile,
  author = {Docker Inc.},
  title = {{Best practices for writing Dockerfiles}},
  howpublished = "\url{https://docs.docker.com/develop/develop-images/dockerfile_best-practices/}",
  note = {Accessed: 2019-04-15}
}

@misc{Kubernetes-Pod,
  author = {The Linux Foundation},
  title = {{Pod Overview}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/}",
  note = {Accessed: 2019-04-16}
}

@misc{Kubernetes-ReplicaSet,
  author = {The Linux Foundation},
  title = {{ReplicaSet}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/}",
  note = {Accessed: 2019-06-04}
}

@misc{Kubernetes-StatefulSet,
  author = {The Linux Foundation},
  title = {{StatefulSets}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/}",
  note = {Accessed: 2019-06-04}
}

@misc{Kubernetes-Deployment,
  author = {The Linux Foundation},
  title = {{Deployments}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/workloads/controllers/deployment/}",
  note = {Accessed: 2019-06-04}
}

@misc{Kubernetes-Service,
  author = {The Linux Foundation},
  title = {{Services}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/services-networking/service/}",
  note = {Accessed: 2019-04-16}
}

@misc{Kubernetes-Volume,
  author = {The Linux Foundation},
  title = {{Volumes}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/storage/volumes/}",
  note = {Accessed: 2019-04-16}
}

@misc{Kubernetes-Namespace,
  author = {The Linux Foundation},
  title = {{Namespaces}},
  howpublished = "\url{https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/}",
  note = {Accessed: 2019-04-16}
}

@misc{HPA,
  author = {{Cloud Native Computing Foundation}},
  title = {{Horizontal Pod Autoscaler}},
  howpublished = "\url{https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/}",
  note = {Accessed: 2019-07-29}
}

@misc{redis,
  author = {Redis Labs},
  title = {{Redis}},
  howpublished = "\url{https://redis.io/}",
  note = {Accessed: 2019-08-08}
}

@misc{VPA,
  author = {Google},
  title = {{Virtical Pod Autoscaler}},
  howpublished = "\url{https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler}",
  note = {Accessed: 2019-07-29}
}


@article{Container-orchestration,
issn = {2325-6095},
abstract = {<p>As compute evolves from bare metal to virtualized environments to containers towards server-less, the efficiency gains have enabled a wide variety of use cases. Organizations have used containers to run long running services, batch processing at scale, control planes, Internet of Things, and Artificial Intelligence workloads. Further, methodologies for software as a service, such as twelve-factor app, emphasize a clean contract with the underlying operating system and maximum portability between execution environments. In this paper, we address a set of capabilities required of a container orchestration platform to embody the design principles as illustrated by twelve factor app design. This paper also provides a non-exhaustive and prescriptive guide to identifying and implementing key mechanisms required in a container orchestration platform. We will cover capabilities such as cluster state management and scheduling, high availability and fault tolerance, security, networking, service discovery, continuous deployment, monitoring, and governance.</p>},
journal = {IEEE Cloud Computing},
pages = {42--48},
volume = {4},
publisher = {IEEE},
number = {5},
year = {2017},
title = {Key Characteristics of a Container Orchestration Platform to Enable a Modern Application},
language = {eng},
author = {Khan, Asif},
keywords = {Containers ; Cloud Computing ; Security ; Fault Tolerant Systems ; Ports (Computers) ; Fault Tolerance ; Microservices ; Containers ; Docker ; Container Orchestration ; Twelve-Factor ; Cluster Management ; Scheduling ; High Availability ; Fault Tolerant ; Container Security ; Service Discovery ; Dynamic Port Mapping ; Application Load Balancing ; Monitoring ; Tracing ; Cyclomatic Complexity ; Code Review ; Continuous Delivery and Deployment ; Computer Science},
}

@article{Docker,
issn = {0167-6423},
abstract = {Cloud applications typically integrate multiple components, each needing a virtualised runtime environment that provides the required software support (e.g., operating system, libraries). This paper shows how TOSCA and Docker can effectively support the orchestration of multi-component applications, even when their runtime specification is incomplete. More precisely, we first introduce a TOSCA-based representation of multi-component applications, and we illustrate how such representation can be exploited to specify only the application-specific components. We then present TosKeriser, a tool for automatically completing TOSCA application specifications, which can automatically discover the Docker-based runtime environments that provide the software support needed by the application components. We also show how we fruitfully exploited TosKeriser in two concrete case studies. Finally, we discuss how the specifications completed by TosKeriser can be automatically orchestrated by already existing TOSCA engines. •We present a TOSCA-based representation of multi-component applications.•Our representation permits specifying only application-specific components.•We present TosKeriser, a tool to automatically complete TOSCA specifications.•Automatically completed specifications can be executed by existing TOSCA engines.•We discuss the usefulness of TosKeriser by means of two concrete case studies.},
journal = {Science of Computer Programming},
pages = {194--213},
volume = {166},
publisher = {Elsevier B.V.},
year = {2018},
title = {Orchestrating incomplete TOSCA applications with Docker},
language = {eng},
author = {Brogi, Antonio and Neri, Davide and Rinaldi, Luca and Soldani, Jacopo},
keywords = {Cloud Applications ; Tosca ; Docker ; Container Reuse},
}

@article{Container,
abstract = {Virtualization technologies have evolved along with the development of computational environments since virtualization offered needed features at that time such as isolation, accountability, resource allocation, resource fair sharing and so on. Novel processor technologies bring to commodity computers the possibility to emulate diverse environments where a wide range of computational scenarios can be run. Along with processors evolution, system developers have created different virtualization mechanisms where each new development enhanced the performance of previous virtualized environments. Recently, operating system-based virtualization technologies captured the attention of communities abroad (from industry to academy and research) because their important improvements on performance area. In this paper, the features of three container-based operating systems virtualization tools (LXC, Docker and Singularity) are presented. LXC, Docker, Singularity and bare metal are put under test through a customized single node HPL-Benchmark and a MPI-based application for the multi node testbed. Also the disk I/O performance, Memory (RAM) performance, Network bandwidth and GPU performance are tested for the COS technologies vs bare metal. Preliminary results and conclusions around them are presented and discussed.},
year = {2017},
title = {Performance Evaluation of Container-based Virtualization for High Performance Computing Environments},
author = {Arango, Carlos and Dernat, R\"{e}my and Sanabria, John},
keywords = {Computer Science - Operating Systems ; Computer Science - Distributed, Parallel, And Cluster Computing ; Computer Science - Performance},
}

@article{ContainerVsVM,
issn = {0167-739X},
abstract = {Web-facing applications are expected to provide certain performance guarantees despite dynamic and continuous workload changes. As a result, application owners are using cloud computing as it offers the ability to dynamically provision computing resources (e.g., memory, CPU) in response to changes in workload demands to meet performance targets and eliminates upfront costs. Horizontal, vertical, and the combination of the two are the possible dimensions that cloud application can be scaled in terms of the allocated resources. In vertical elasticity as the focus of this work, the size of virtual machines (VMs) can be adjusted in terms of allocated computing resources according to the runtime workload. A commonly used vertical resource elasticity approach is realized by deciding based on resource utilization, named capacity-based. While a new trend is to use the application performance as a decision making criterion, and such an approach is named performance-based. This paper discusses these two approaches and proposes a novel hybrid elasticity approach that takes into account both the application performance and the resource utilization to leverage the benefits of both approaches. The proposed approach is used in realizing vertical elasticity of memory (named as vertical memory elasticity), where the allocated memory of the VM is auto-scaled at runtime. To this aim, we use control theory to synthesize a feedback controller that meets the application performance constraints by auto-scaling the allocated memory, i.e., applying vertical memory elasticity. Different from the existing vertical resource elasticity approaches, the novelty of our work lies in utilizing both the memory utilization and application response time as decision making criteria. To verify the resource efficiency and the ability of the controller in handling unexpected workloads, we have implemented the controller on top of the Xen hypervisor and performed a series of experiments using the RUBBoS interactive benchmark application, under synthetic and real workloads including Wikipedia and FIFA. The results reveal that the hybrid controller meets the application performance target with better performance stability (i.e., lower standard deviation of response time), while achieving a high memory utilization (close to 83\%), and allocating less memory compared to all other baseline controllers. A feedback controller for vertically scale the memory of cloud applications is proposed.The controller is able to tune the memory in order to meet the desired performance.The application performance and memory utilization are used as decision making criteria.The feedback controller guarantees the stability of the cloud application.The results show the efficiency in memory usage when the feedback controller is used.},
journal = {Future Generation Computer Systems},
pages = {57--72},
volume = {65},
publisher = {Elsevier B.V.},
number = {C},
year = {2016},
title = {A hybrid cloud controller for vertical memory elasticity: A control-theoretic approach},
language = {eng},
author = {Farokhi, Soodeh and Jamshidi, Pooyan and Bayuh Lakew, Ewnetu and Brandic, Ivona and Elmroth, Erik},
keywords = {Cloud Computing ; Control Theory ; Vertical Memory Elasticity ; Interactive Application Performance ; Memory Utilization},
}

@article{CoutinhoEmanuel2015Eicc,
author = {Coutinho, Emanuel and Carvalho Sousa, Flávio and Rego, Paulo and Gomes, Danielo and Souza, Jos\"{e}},
keywords = {Cloud computing ; Elasticity ; Systematic review ; Metrics ; Strategies},
issn = {0003-4347},
abstract = {Cloud computing is now a well-consolidated paradigm for on-demand services provisioning on a pay-as-you-go model. Elasticity, one of the major benefits required for this computing model, is the ability to add and remove resources “on the fly” to handle the load variation. Although many works in literature have surveyed cloud computing and its features, there is a lack of a detailed analysis about elasticity for the cloud. As an attempt to fill this gap, we propose this survey on cloud computing elasticity based on an adaptation of a classic systematic review. We address different aspects of elasticity, such as definitions, metrics and tools for measuring, evaluation of the elasticity, and existing solutions. Finally, we present some open issues and future directions. To the best of our knowledge, this is the first study on cloud computing elasticity using a systematic review approach.},
journal = {annals of telecommunications - annales des t\"{e}l\"{e}communications},
pages = {289--309},
volume = {70},
publisher = {Springer Paris},
number = {7},
year = {2015},
title = {Elasticity in cloud computing: a survey},
language = {eng},
address = {Paris},
}

@article{Lorido-BotranTania2014ARoA,
author = {Lorido-Botran, Tania and Miguel-Alonso, Jose and Lozano, Jose},
keywords = {Cloud computing ; Scalable applications ; Auto-scaling ; Service level agreement},
issn = {1570-7873},
abstract = {Cloud computing environments allow customers to dynamically scale their applications. The key problem is how to lease the right amount of resources, on a pay-as-you-go basis. Application re-dimensioning can be implemented effortlessly, adapting the resources assigned to the application to the incoming user demand. However, the identification of the right amount of resources to lease in order to meet the required Service Level Agreement, while keeping the overall cost low, is not an easy task. Many techniques have been proposed for automating application scaling. We propose a classification of these techniques into five main categories: static threshold-based rules, control theory, reinforcement learning, queuing theory and time series analysis. Then we use this classification to carry out a literature review of proposals for auto-scaling in the cloud.},
journal = {Journal of Grid Computing},
pages = {559--592},
volume = {12},
publisher = {Springer Netherlands},
number = {4},
year = {2014},
title = {A Review of Auto-scaling Techniques for Elastic Applications in Cloud Environments},
language = {eng},
address = {Dordrecht},
}

@article{Al-DhuraibiYahya2018EiCC,
author = {Al-Dhuraibi, Yahya and Paraiso, Fawaz and Djarallah, Nabil and Merle, Philippe},
keywords = {Elasticity ; Cloud Computing ; Virtualization ; Containers ; Scalability ; Operating Systems ; Taxonomy ; Elasticity ; Cloud Computing ; Auto-Scaling ; Resource Provision ; Scalability ; Containers ; Engineering},
issn = {1939-1374},
abstract = {<p>Elasticity is a fundamental property in cloud computing that has recently witnessed major developments. This article reviews both classical and recent elasticity solutions and provides an overview of containerization, a new technological trend in lightweight virtualization. It also discusses major issues and research challenges related to elasticity in cloud computing. We comprehensively review and analyze the proposals developed in this field. We provide a taxonomy of elasticity mechanisms according to the identified works and key properties. Compared to other works in literature, this article presents a broader and detailed analysis of elasticity approaches and is considered as the first survey addressing the elasticity of containers.</p>},
journal = {IEEE Transactions on Services Computing},
pages = {430--447},
volume = {11},
publisher = {IEEE},
number = {2},
year = {2018},
title = {Elasticity in Cloud Computing: State of the Art and Research Challenges},
language = {eng},
}

@inproceedings{scalar,
author = {Delnat, Wito and Truyen, Eddy and Rafique, Ansar and Van Landuyt, Dimitri, Joosen, Wouter},
keywords = {Databases ; Containers ; Measurement ; Load Management ; Standards ; Testing ; Gold ; Container Orchestration ; Autoscaling ; Experimentation Exemplar},
abstract = {<p>Although a considerable amount of research exists on auto-scaling of database clusters, the design of an effective auto-scaling strategy requires fine-grained tailoring towards the specific application scenario. This paper presents an easy-to-use and extensible workbench exemplar, named K8-Scalar (Kube-Scalar), which allows researchers to implement and evaluate different self-adaptive approaches to autoscaling container-orchestrated services. The workbench is based on Docker, a popular technology for easing the deployment of containerized software that also has been positioned as an enabler for reproducible research. The workbench also relies on a container orchestration framework: Kubernetes (K8s), the de-facto industry standard for orchestration and monitoring of elastically scalable container-based services. Finally, it integrates and extends Scalar, a generic testbed for evaluating the scalability of large-scale systems with support for evaluating the performance of autoscalers for database clusters. The paper discusses (i) the architecture and implementation of K8-Scalar and how a particular autoscaler can be plugged in, (ii) sketches the design of a Riemann-based autoscaler for database clusters, (iii) illustrates how to design, setup and analyze a series of experiments to configure and evaluate the performance of this autoscaler for a particular database (i.e., Cassandra) and a particular workload type, and (iv) validates the effectiveness of K8-Scalar as a workbench for accurately comparing the performance of different auto-scaling strategies.</p>},
publisher = {ACM},
booktitle = {Proceedings of SEAMS 2018},
isbn = {9781450357159},
year = {2018},
title = {K8-Scalar: A Workbench to Compare Autoscalers for Container-Orchestrated Database Clusters},
language = {eng},
}

@inproceedings{caravel,
    author = {Umesh Deshpande},
    title = {Caravel: Burst Tolerant Scheduling for Containerized Stateful Applications},
    booktitle = {Proceedings of ICDCS 2019},
    publisher = {IEEE},
    month = nov,
    year = 2019
}

@article{lmaas,
author = {Rafique, Ansar and Van Landuyt, Dimitri and Joosen, Wouter},
keywords = {Policy-based middleware ; NoSQL data stores ; Polyglot persistence ; Multi-tenant SaaS ; Federated cloud storage ; Data encryption},
issn = {1570-7873},
abstract = {NoSQL data stores are often combined to address different requirements within the same application. The implication of this trend is particularly important and relevant in the context of multi-tenant SaaS applications where tenants commonly have different storage- and privacy-related requirements and thus they desire to customize the storage setup according to their specific needs. Consequently, application developers are increasingly combining storage resources: on-premise and public cloud resources in a hybrid cloud setup, different external public cloud storage resources and providers in a federated cloud storage setup, etc. The consequences of these trends are twofold: (i) application developers and SaaS providers have to deal with heterogeneous technologies, different APIs, and implement complex storage logic (to address different requirements of tenants), all within the application layer; and (ii) storage architectures have become less rigid, and techniques are required to flexibly change the storage configuration of running applications, up to the level of individual service requests. To address these challenges, we present PERSIST, a middleware architecture that (i) externalizes the complexity of a federated cloud storage architecture and the complex storage logic from the SaaS application to storage policies, allows tenants to enforce different storage- and privacy-related requirements at a fine-grained level; and (ii) supports the dynamic (re)configurability of the underlying federated cloud storage architecture. Application-specific policies can be customized by individual tenants at run time, and PERSIST offers support for run-time cross-provider polyglot persistence and the confidentiality of sensitive data through encryption. We have validated PERSIST in a working prototype implementation. Our extensive evaluation efforts show (i) the accomplished reduction in the required development effort to support complex storage policies, (ii) the reduction in cost/effort to change the data storage architecture itself, and finally (iii) the acceptability of the performance overhead (around 6\% for insert, and 2\% for read, update and delete transactions).},
journal = {Journal of Grid Computing},
pages = {165--194},
volume = {16},
publisher = {Springer Netherlands},
number = {2},
year = {2018},
title = {PERSIST: Policy-Based Data Management Middleware for Multi-Tenant SaaS Leveraging Federated Cloud Storage},
language = {eng},
address = {Dordrecht},
}



@misc{heapster,
  author = {Kubernetes},
  title = {Heapster},
  howpublished = {\url{https://github.com/kubernetes-retired/heapster/}},
  note = {Accessed: 2019-08-02}
}

@misc{grafana,
  author = {Grafana Labs},
  title = {Grafana},
  howpublished = {\url{https://grafana.com/}},
  note = {Accessed: 2019-08-02}
}

@misc{influxdb,
  title = {InfluxDB},
  author = {InfluxData},
  howpublished = {\url{https://www.influxdata.com/}},
  note = {Accessed: 2019-08-02}
}

@misc{heapster-influxdb-grafana,
  title = {How to Utilize the ``Heapster + InfluxDB + Grafana" Stack in Kubernetes for Monitoring Pods},
  author = {Kublr Team},
  year = {2017},
  howpublished = {\url{https://kublr.com/blog/how-to-utilize-the-heapster-influxdb-grafana-stack-in-kubernetes-for-monitoring-pods/}},
  note = {Accessed: 2019-08-02}
}

@misc{kubelet,
  title = {Kubelet},
  author = {Kubernetes},
  howpublished = {\url{https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/}},
  note = {Accessed: 2019-08-02}
}

@misc{timeseriesdb,
  title = {What is a time series database?},
  author = {InfluxData},
  howpublished = {\url{https://www.influxdata.com/time-series-database/}},
  note = {Accessed: 2019-08-02}
}

@misc{grafana-github,
  title = {Grafana GitHub page},
  author = {InfluxData},
  howpublished = {\url{https://github.com/grafana/grafana}},
  note = {Accessed: 2019-08-02}
}

@misc{scalar-github,
  title = {K8-Scalar},
  author = {Delnat, Wito and Truyen, Eddy},
  howpublished = {\url{https://github.com/k8-scalar/k8-scalar}},
  note = {Accessed: 2019-08-02}
}

@misc{hpa-algorithm-details,
  title = {Horizontal Pod Autoscaler: Algorithm Details},
  author = {{Cloud Native Computing Foundation}},
  howpublished = {\url{https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details}},
  note = {Accessed: 2019-08-12}
}

@misc{hpa-cooldown-delay,
  title = {Horizontal Pod Autoscaler: Support for cooldown/delay},
  author = {{Cloud Native Computing Foundation}},
  howpublished = {\url{https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-cooldown-delay}},
  note = {Accessed: 2019-08-12}
}

@misc{vpa-limitations,
  title = {Vertical Pod Autoscaler: Known limitations},
  author = {{Cloud Native Computing Foundation}},
  howpublished = {\url{https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#known-limitations}},
  note = {Accessed: 2019-10-22}
}


@misc{scalar-github-overview,
  title = {K8-Scalar},
  author = {Delnat, Wito and Truyen, Eddy},
  howpublished = {\url{https://github.com/k8-scalar/k8-scalar/blob/master/docs/overview.md}},
  note = {Accessed: 2019-08-02}
}

@misc{scalar-github-load,
  title = {K8-Scalar experiment properties},
  author = {Delnat, Wito and Truyen, Eddy},
  howpublished = {\url{https://github.com/k8-scalar/k8-scalar/blob/master/development/example-experiment/etc/experiment.properties}},
  note = {Accessed: 2019-08-02}
}

@misc{cassandra,
  title = {Apache Cassandra. Understanding the architecture.},
  howpublished = {\url{https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/architecture/archTOC.html}},
  note = {Accessed: 2019-08-02}
}

@misc{WitoThesis,
author = {Delnat, Wito},
abstract = {Het toenemend gebruik van container-gebaseerde virtualisatietechnologieën groeit snel. Het is een onderdeel van een grotere evolutie naar cloud computing. E\"{e}n van de redenenen achter dit toenemend gebruik is de verhoogde beschikbaarheid en flexibiliteit van applicaties. Hierbij zijn autoschalers essentieel. Autoschalers kunnen op een autonome manier hulpmiddelen provisioneren of deprovisioneren. Deze masterproef beschrijft een autonomische autoschaler voor het meest populaire container orchestratie platform, Kubernetes. De autoschaler biedt een oplossing voor twee limitaties bij het autoschalen in Kubernetes. Ten eerste geeft Kubernetes geen ondersteuning voor applicaties met persistente data, de zogenaamde StatefulSets. Ten tweede worden enkel simpele strategieën ondersteunt. Het is bijvoorbeeld niet mogelijk om berekeningen met metrieken uit te voeren. De autoschaler kan gedecomposeerd worden in drie microservices die respectievelijk verantwoordelijk zijn voor het verzamelen van metrieken, het analyseren hiervan en het uitvoeren van schalingsoperaties. De analyse gebeurt op een reactieve manier. Door middel van experimenten werd er aangetoond dat autoschaling zinvol en haalbaar is voor NoSQL clusters in Kubernetes. Verder werden alternatieve autoschalingsstrategieën onderzocht om het aantal service level agreement (SLA) schendingen in het populaire NoSQL opslagsysteem Apache Cassandra te verminderen in vergelijking met de strategie die de standaardversie van Kubernetes aanbiedt. Deze alternatieve autoschalingsstrategieën konden rekening houden met de opstarttijd van de Cassandra instantie en elimineerden uitschieters in de metrieken. Er werd hiervoor een transformatie van SLA naar metrieken bepaald in de vorm van kritische punten. Deze kritische punten bepalen de drempelwaarde van de bottlenecks van hulpmiddelen voor een SLA. Een voordeel van deze kritische punten is dat ze geoptimaliseerd kunnen worden indien de belasting die een applicatie genereert bekend is, of voorspelbaar.},
publisher = {KU Leuven. Faculteit Ingenieurswetenschappen},
year = {2017},
title = {Monitoring en elastisch schalen van NoSQL clusters in een container georchestreerde omgeving},
address = {Leuven},
}

@misc{saas-app,
author = {Jacobs, Andr\"{e}},
abstract = {Voor aanbieders van Software as a Service is het een constante uitdaging om flexibel en snel computing resources te (her)alloceren en hardware maximaal te benutten. Hiervoor wordt door SaaS-providers gebruik gemaakt van een multi-tenant archi- tectuur. Hierdoor is er nood aan adaptieve performantie-isolatie tussen tenants om SLA's en veiligheid te kunnen garanderen. Huidige oplossingen hiervoor zijn gebaseerd op tenant bewuste controlelussen. Het probleem met deze oplossingen is dat configuratie niet stabiel is onder veranderende workload. Dit betekent dat configuratie voor eenzelfde SLA moet aangepast worden wanneer globale resource consumptie veranderd. Er is de laatste jaren een grote stijging in populariteit voor containertechnologie zoals Docker en container orchestratiesystemen, bijvoorbeeld Kubernetes, om kost-efficiëntie, schaalbaarheid en elasticiteit te verhogen en verbete- ren. In deze thesis wordt onderzocht of het probleem van adaptieve performantie isolatie kan mappen op de resource management concepten van Kubernetes. Er wordt onderzocht of QoS differentiatie mogelijk is in Kubernetes en daarna wordt dit resultaat gebruikt om te bepalen of het mogelijk is om een multi-tenant SaaS applicatie te deployen met eenvoudige configuratie die geen aanpassing nodig heeft bij variabele workload. Op basis van experimenten uitgevoerd op een Kubernetes cluster met behulp van een zelf ontwikkelde artificiële SaaS-applicatie is geconclu- deerd dat het mogelijk is om eenvoudig aan dynamische resource allocatie te doen bij veranderende workloads door middel van horizontaal schalen in Kubernetes. Dit vereist wel goede kalibratie op voorhand en is er een trade-off tussen kost-efficiëntie en verfijnde resource allocatie.},
publisher = {KU Leuven. Faculteit Ingenieurswetenschappen},
year = {2017},
title = {Haalbaarheidsstudie van container orchestratie voor performantie-isolatie in multi-tenant SaaS-applicaties},
address = {Leuven},
}

@inproceedings{TruyenEddy2019Pooc,
author = {Truyen, Eddy and Van Landuyt, Dimitri and Lagaisse, Bert and Joosen, Wouter},
keywords = {Science & Technology},
url = {$$Uhttps://lirias.kuleuven.be/retrieve/536942$$Dsac2019-1798.pdf [freely available]},
abstract = {The most preferred approach in the literature on service-level objectives for multi-tenant databases is to group tenants according to their SLA class in separate database processes and find optimal co-placement of tenants across a cluster of nodes. To implement performance isolation between co-located database processes, request scheduling is preferred over hypervisor-based virtualization that introduces a significant performance overhead. A relevant question is whether the more light-weight container technology such as Docker is a viable alternative for running high-end performance database workloads. Moreover, the recent uprise and industry adoption of container orchestration (CO) frameworks for the purpose of automated placement of cloud-based applications raises the question what is the additional performance overhead of CO frameworks in this context. In this paper, we evaluate the performance overhead introduced by Docker engine and two representative CO frameworks, Docker Swarm and Kubernetes, when running and managing a CPU-bound Cassandra workload in OpenStack. Firstly, we have found that Docker engine deployments that run in host mode exhibit negligible performance overhead in comparison to native OpenStack deployments. Secondly, we have found that virtual IP networking introduces a substantial overhead in Docker Swarm and Kubernetes due to virtual network bridges when compared to Docker engine deployments. This demands for service networking approaches that run in true host mode but offer support for network isolation between containers. Thirdly, volume plugins for persistent storage have a large impact on the overall resource model of a database workload; more specifically, we show that a CPU-bound Cassandra workload changes into an I/O-bound workload in both Docker Swarm and Kubernetes because their local volume plugins introduce a disk I/O performance bottleneck that does not appear in Docker engine deployments. These findings imply that solved placement decisions for native or Docker engine deployments cannot be reused for Docker Swarm and Kubernetes.},
journal = {https://doi.org/10.1145/3297280.3297536},
volume = {Part F147772},
publisher = {ACM},
year = {2019},
title = {Performance overhead of container orchestration frameworks for management of multi-tenant database deployments},
language = {eng},
}

